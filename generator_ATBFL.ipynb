{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1d1a5b",
   "metadata": {},
   "source": [
    "# BioDCASE ATBFL Dataset Generator for BaseAL\n",
    "\n",
    "This notebook converts the BioDCASE Challenge 4 (ATBFL-derived) strongly-labeled whale call dataset into BaseAL format.\n",
    "\n",
    "## Input format (BioDCASE release)\n",
    "- Annotations live in:\n",
    "  - `train/annotations/*.csv`\n",
    "  - `validation/annotations/*.csv`\n",
    "- Each annotation row is an event with absolute timestamps:\n",
    "  `(dataset, filename, annotation, annotator, low_frequency, high_frequency, start_datetime, end_datetime)`\n",
    "\n",
    "## Output format (BaseAL)\n",
    "```\n",
    "ATBFL_BASEAL/\n",
    "├── audio_flat/                 # symlinks to original long recordings\n",
    "├── data/<MODEL>/               # segmented audio clips\n",
    "├── embeddings/<MODEL>/         # per-segment embeddings (.npy)\n",
    "├── labels.csv                  # filename, label, validation\n",
    "└── metadata.csv                # segment-level metadata\n",
    "```\n",
    "\n",
    "## Labeling rule\n",
    "- We convert absolute event timestamps into seconds relative to the recording start time (parsed from the audio filename).\n",
    "- For each model segment window, we collect all overlapping events and assign a semicolon-separated multi-label (e.g., `bma;bmb`).\n",
    "- Segments with no overlapping events get `NO_EVENT_LABEL`.\n",
    "\n",
    "Notes:\n",
    "- This generator follows the same bacpipe-based embedding pipeline as `generator_ESC50.ipynb`.\n",
    "- The official BioDCASE split is preserved: all segments from `validation/` become `validation=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aaf4c0",
   "metadata": {},
   "source": [
    "### Overview Notes\n",
    "* Data from the task 2 \"Supervised Detection of Strongly-Labelled Whale Calls\" of the BioDCASE 2025 challenge. It has been derived from the ATBFL library\n",
    "* ATBFL is one of the largest annotated datasets in marine bioacoustics, gathering blue and fin whale recordings around Antarctica from 2005 to 2017\n",
    "* 6591 audio files (6004 in train set + 587 in validation set) totaling 1880 hours of recordings from 11 different deployments organized in site-year datasets (eg, kerguelen-2015)\n",
    "* 11 CSV annotation files named after each corresponding site-year dataset\n",
    "\n",
    "### Annotation Format\n",
    "* Annotations come in the form `(dataset,filename,annotation,annotator,low_frequency,high_frequency,start_datetime,end_datetime)`\n",
    "* Label set: `{bma, bmb, bmz, bmd, bpd, bp20, bp20plus}` (the more readable version used)`{BmA, BmB, BmZ, BmD, BpD, Bp20, Bp20plus}` \n",
    "* One single annotator per dataset but a same annotator may have annotated several datasets\n",
    "* Calls may overlap, so the set up is multi-class and multi-label: one file or segment of file file is likely to contain several classes\n",
    "* 7 classes are provided but the evaluation will only take 3 into account as calls can be gathered by similarity (see below)\n",
    "\n",
    "### Call Descriptions\n",
    "\n",
    "**Downsweeps:** BmA, BmB, and BmZ calls are specific to blue whales (Balaenoptera musculus intermedia, Bm), while Bp20 and Bp20Plus calls are characteristic of fin whales (Balaenoptera physalus quoyi, Bp).\n",
    "\n",
    "**ABZ Calls:** As described by Miller et al. (see \"Related Works\"), BmA calls consist of a constant-frequency tone between 25 and 28 Hz, without additional units. BmB calls are similar but followed by a partial or full inter-tone downsweep. BmZ calls contain two tonal units: A (higher frequency) and C (lower frequency). Occasionally, a B downsweep unit appears between them, forming a \"Z\" shape on spectrograms.\n",
    "\n",
    "**Bp Calls:** Bp20 and Bp20Plus vocalizations are pulsed calls with peak energy at 20 Hz (Bp20) and additional energy at higher frequencies (80–100 Hz) in Bp20Plus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8340287",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "To download the data, run \n",
    "\n",
    "```bash\n",
    "wget https://zenodo.org/records/15092732/files/biodcase_development_set.zip\n",
    "unzip biodcase_development_set.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.embeddings import generate_embeddings, initialise\n",
    "from utils.helpers import convert_for_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939ed48",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set paths and model choice. `perch_v2` only runs on Linux/WSL; `birdnet` is usually easier to run everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbedfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== User Config ==============================\n",
    "MODEL = \"perch_v2\"  # or: \"birdnet\"\n",
    "\n",
    "# Where the BioDCASE release folder lives (must contain train/ and validation/), environment variable LOCAL_SCRATCH can be set to point to a local scratch directory for faster access in HPC environments\n",
    "LOCAL_SCRATCH = os.getenv(\"LOCAL_SCRATCH\")\n",
    "\n",
    "if LOCAL_SCRATCH:\n",
    "    BIODCASE_ROOT = Path(LOCAL_SCRATCH) / \"biodcase\" / \"biodcase_development_set\"\n",
    "    DATASET_PATH = Path(LOCAL_SCRATCH) / \"biodcase\" / \"ATBFL_BASEAL\"\n",
    "else:\n",
    "    BIODCASE_ROOT = Path(\"biodcase_development_set\")\n",
    "    DATASET_PATH = Path(\"ATBFL_BASEAL\")\n",
    "\n",
    "# Segment labeling\n",
    "NO_EVENT_LABEL = \"no_call\"\n",
    "LABEL_SEPARATOR = \";\"\n",
    "MIN_OVERLAP_SEC = 0.0  # absolute overlap threshold\n",
    "\n",
    "# Optional: limit number of recordings for a quick dry-run (set to None for full)\n",
    "MAX_RECORDINGS = None  # e.g., 50\n",
    "\n",
    "# ============================== End Config =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db316532",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH.mkdir(parents=True, exist_ok=True)\n",
    "SEG_PATH = DATASET_PATH / \"data\" / MODEL\n",
    "EMB_PATH = DATASET_PATH / \"embeddings\" / MODEL\n",
    "FLAT_AUDIO = DATASET_PATH / \"audio_flat\"\n",
    "SEG_PATH.mkdir(parents=True, exist_ok=True)\n",
    "EMB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "FLAT_AUDIO.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"BioDCASE root: {BIODCASE_ROOT}\")\n",
    "print(f\"Output path:   {DATASET_PATH}\")\n",
    "print(f\"Model:         {MODEL}\")\n",
    "print(f\"Flat audio:    {FLAT_AUDIO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f53d19e",
   "metadata": {},
   "source": [
    "## Load annotations and build file-level metadata\n",
    "\n",
    "We load both train and validation annotation CSVs, parse datetimes, and compute event onset/offset in seconds relative to the recording start time.\n",
    "\n",
    "Recording start time is inferred from the BioDCASE filename pattern:\n",
    "- Example filename: `2015-02-04T03-00-00_000.wav`\n",
    "- Recording start assumed: `2015-02-04T03:00:00+00:00`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c46936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_recording_start_utc(filename: str) -> pd.Timestamp | None:\n",
    "    name = Path(filename).name\n",
    "    # Common ATBFL naming: <YYYY-MM-DD>T<HH-MM-SS>_<...>.wav\n",
    "    prefix = name.split(\"_\", 1)[0]\n",
    "    if \"T\" not in prefix:\n",
    "        return None\n",
    "    try:\n",
    "        date_part, time_part = prefix.split(\"T\", 1)\n",
    "        time_part = time_part.replace(\"-\", \":\")\n",
    "        # Force UTC; annotation datetimes include +00:00\n",
    "        return pd.Timestamp(f\"{date_part}T{time_part}+00:00\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _load_annotation_csvs(biodcase_root: Path) -> pd.DataFrame:\n",
    "    records: list[pd.DataFrame] = []\n",
    "    for split in [\"train\", \"validation\"]:\n",
    "        ann_dir = biodcase_root / split / \"annotations\"\n",
    "        if not ann_dir.exists():\n",
    "            raise FileNotFoundError(f\"Missing annotations directory: {ann_dir}\")\n",
    "        csvs = sorted(ann_dir.glob(\"*.csv\"))\n",
    "        if not csvs:\n",
    "            raise FileNotFoundError(f\"No annotation CSVs found under: {ann_dir}\")\n",
    "        for p in csvs:\n",
    "            df = pd.read_csv(p)\n",
    "            df[\"split\"] = split\n",
    "            df[\"source_csv\"] = p.name\n",
    "            records.append(df)\n",
    "    out = pd.concat(records, ignore_index=True, sort=False)\n",
    "    return out\n",
    "\n",
    "\n",
    "ann = _load_annotation_csvs(BIODCASE_ROOT)\n",
    "print(f\"Loaded {len(ann):,} annotation rows\")\n",
    "print(ann.columns.tolist())\n",
    "ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse datetimes and compute relative onsets/offsets in seconds\n",
    "ann = ann.copy()\n",
    "ann[\"start_datetime\"] = pd.to_datetime(ann[\"start_datetime\"], errors=\"coerce\")\n",
    "ann[\"end_datetime\"] = pd.to_datetime(ann[\"end_datetime\"], errors=\"coerce\")\n",
    "ann[\"recording_start\"] = ann[\"filename\"].apply(_parse_recording_start_utc)\n",
    "\n",
    "ann[\"onset_sec\"] = (ann[\"start_datetime\"] - ann[\"recording_start\"]).dt.total_seconds()\n",
    "ann[\"offset_sec\"] = (ann[\"end_datetime\"] - ann[\"recording_start\"]).dt.total_seconds()\n",
    "\n",
    "# Basic sanity filtering\n",
    "ann = ann.dropna(subset=[\"dataset\", \"filename\", \"annotation\", \"onset_sec\", \"offset_sec\"]).copy()\n",
    "ann = ann[(ann[\"offset_sec\"] > ann[\"onset_sec\"]) & (ann[\"onset_sec\"] >= 0)].copy()\n",
    "\n",
    "print(f\"After filtering: {len(ann):,} valid events\")\n",
    "ann[[\"split\", \"dataset\", \"filename\", \"annotation\", \"onset_sec\", \"offset_sec\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eee6b2",
   "metadata": {},
   "source": [
    "### Build per-recording rows\n",
    "\n",
    "We aggregate events by `(split, dataset, filename)` into one row per recording. Each row contains:\n",
    "- the original audio path\n",
    "- duration in seconds (read from the audio header)\n",
    "- `events`: list of `[onset, offset]` in seconds\n",
    "- `event_labels`: list of labels aligned with `events`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _audio_duration_sec(path: Path) -> float | None:\n",
    "    try:\n",
    "        info = torchaudio.info(os.fspath(path))\n",
    "        if info.num_frames <= 0 or info.sample_rate <= 0:\n",
    "            return None\n",
    "        return float(info.num_frames) / float(info.sample_rate)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "group_cols = [\"split\", \"dataset\", \"filename\"]\n",
    "rows: list[dict] = []\n",
    "\n",
    "n_groups = ann[group_cols].drop_duplicates().shape[0]\n",
    "for (split, dataset, filename), g in tqdm(\n",
    "    ann.groupby(group_cols, sort=False),\n",
    "    total=n_groups,\n",
    "    desc=\"Aggregating recordings\",\n",
    "    unit=\"rec\",\n",
    "    ):\n",
    "    audio_path = BIODCASE_ROOT / split / \"audio\" / str(dataset) / str(filename)\n",
    "    duration = _audio_duration_sec(audio_path) if audio_path.exists() else None\n",
    "\n",
    "    events = g[[\"onset_sec\", \"offset_sec\"]].to_numpy(dtype=float).tolist()\n",
    "    event_labels = g[\"annotation\"].astype(str).tolist()\n",
    "\n",
    "    flat_name = f\"{dataset}__{filename}\"\n",
    "    rows.append(\n",
    "        {\n",
    "            \"split\": split,\n",
    "            \"dataset\": str(dataset),\n",
    "            \"source_filename\": str(filename),\n",
    "            \"source_audio_path\": os.fspath(audio_path),\n",
    "            \"flat_filename\": flat_name,\n",
    "            \"filepath\": os.fspath(FLAT_AUDIO / flat_name),\n",
    "            \"length\": duration,\n",
    "            \"detected_events\": events,\n",
    "            \"detected_event_labels\": event_labels,\n",
    "            \"annotator\": g[\"annotator\"].iloc[0] if \"annotator\" in g.columns else None,\n",
    "        }\n",
    "    )\n",
    "\n",
    "file_df = pd.DataFrame(rows)\n",
    "missing_audio = int(file_df[\"length\"].isna().sum())\n",
    "print(f\"Recordings: {len(file_df):,}\")\n",
    "print(f\"Missing/unreadable audio: {missing_audio:,}\")\n",
    "\n",
    "if MAX_RECORDINGS is not None and len(file_df) > MAX_RECORDINGS:\n",
    "    file_df = file_df.sample(n=MAX_RECORDINGS, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Subsampled to {len(file_df):,} recordings\")\n",
    "\n",
    "file_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670644f5",
   "metadata": {},
   "source": [
    "## Create a flat audio directory (symlinks)\n",
    "\n",
    "`generate_embeddings()` only scans a single directory (non-recursive). We create `audio_flat/` with symlinks to the original recordings.\n",
    "\n",
    "Segment and embedding filenames will be derived from these flat filenames (e.g., `<dataset>__<filename>_000_003.wav`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509fdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Clear existing files in audio_flat (best-effort)\n",
    "for p in FLAT_AUDIO.glob(\"*\"):\n",
    "    try:\n",
    "        if p.is_symlink() or p.is_file():\n",
    "            p.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "created = 0\n",
    "skipped = 0\n",
    "resolved_names: list[str | None] = []\n",
    "\n",
    "for _, row in tqdm(file_df.iterrows(), total=len(file_df), desc=\"Linking audio\"):\n",
    "    src = Path(row[\"source_audio_path\"])\n",
    "    if not src.exists():\n",
    "        resolved_names.append(None)\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    base_name = str(row[\"flat_filename\"])\n",
    "    name = base_name\n",
    "    k = 1\n",
    "    while (FLAT_AUDIO / name).exists():\n",
    "        existing = FLAT_AUDIO / name\n",
    "        try:\n",
    "            if existing.is_symlink() and existing.resolve() == src.resolve():\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "        k += 1\n",
    "        name = f\"{k}__{base_name}\"\n",
    "\n",
    "    dst = FLAT_AUDIO / name\n",
    "    if not dst.exists():\n",
    "        try:\n",
    "            os.symlink(os.fspath(src.resolve()), os.fspath(dst))\n",
    "        except OSError:\n",
    "            shutil.copy2(src, dst)\n",
    "    created += 1\n",
    "    resolved_names.append(name)\n",
    "\n",
    "file_df = file_df.copy()\n",
    "file_df[\"flat_filename\"] = resolved_names\n",
    "file_df = file_df.dropna(subset=[\"flat_filename\"]).reset_index(drop=True)\n",
    "file_df[\"filepath\"] = file_df[\"flat_filename\"].map(lambda x: os.fspath(FLAT_AUDIO / str(x)))\n",
    "\n",
    "print(f\"Created {created} links/copies; skipped {skipped} missing sources\")\n",
    "print(f\"Flat audio files: {len(list(FLAT_AUDIO.glob('*.wav'))):,}\")\n",
    "file_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aee65a",
   "metadata": {},
   "source": [
    "## Generate segments and embeddings\n",
    "\n",
    "This step can be very slow for the full ATBFL dataset (many hours of audio). Consider setting `MAX_RECORDINGS` for testing first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a44982",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = initialise(model_name=MODEL)\n",
    "segment_duration = float(embedder.model.segment_length) / float(embedder.model.sr)\n",
    "print(f\"Model segment duration: {segment_duration:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = generate_embeddings(\n",
    "    audio_dir=FLAT_AUDIO,\n",
    "    embedder=embedder,\n",
    "    model_name=MODEL,\n",
    "    segments_dir=SEG_PATH,\n",
    "    output_dir=EMB_PATH\n",
    " )\n",
    "print(f\"\\nProcessed {len(embeddings)} recordings in flat directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845da8ac",
   "metadata": {},
   "source": [
    "## Build segment-level metadata and labels\n",
    "\n",
    "We reproduce the segment windowing used by bacpipe (integer seconds derived from the model window length), then label each segment by overlapping ATBFL events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f914092",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class SegmentConfig:\n",
    "    \"\"\"Configuration for segment labeling.\"\"\"\n",
    "\n",
    "    segment_duration: float\n",
    "    min_overlap: float = 0.0\n",
    "    no_event_label: str = \"no_call\"\n",
    "    label_separator: str = \";\"\n",
    "\n",
    "\n",
    "def _event_overlaps(\n",
    "    event_onset: float,\n",
    "    event_offset: float,\n",
    "    seg_start: float,\n",
    "    seg_end: float,\n",
    "    config: SegmentConfig,\n",
    " ) -> bool:\n",
    "    overlap_start = max(event_onset, seg_start)\n",
    "    overlap_end = min(event_offset, seg_end)\n",
    "    overlap = max(0.0, overlap_end - overlap_start)\n",
    "    return overlap > 0.0 and overlap >= config.min_overlap\n",
    "\n",
    "\n",
    "def _unique_preserve_order(items: list[str]) -> list[str]:\n",
    "    seen: set[str] = set()\n",
    "    out: list[str] = []\n",
    "    for it in items:\n",
    "        if it not in seen:\n",
    "            seen.add(it)\n",
    "            out.append(it)\n",
    "    return out\n",
    "\n",
    "\n",
    "cfg = SegmentConfig(\n",
    "    segment_duration=segment_duration,\n",
    "    min_overlap=float(MIN_OVERLAP_SEC),\n",
    "    no_event_label=NO_EVENT_LABEL,\n",
    "    label_separator=LABEL_SEPARATOR,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_rows: list[dict] = []\n",
    "\n",
    "for _, row in tqdm(file_df.iterrows(), total=len(file_df), desc=\"Splitting into segments\", unit=\"rec\"):\n",
    "    length = row[\"length\"]\n",
    "    if length is None or (isinstance(length, float) and np.isnan(length)):\n",
    "        continue\n",
    "\n",
    "    stem = Path(row[\"flat_filename\"]).stem\n",
    "    events = np.asarray(row[\"detected_events\"], dtype=float)\n",
    "    event_labels = [str(x) for x in row[\"detected_event_labels\"]]\n",
    "\n",
    "    n_segments = int(np.ceil(float(length) / cfg.segment_duration))\n",
    "    for i in range(n_segments):\n",
    "        seg_start = float(i) * cfg.segment_duration\n",
    "        seg_end = float(i + 1) * cfg.segment_duration\n",
    "\n",
    "        # Mirror bacpipe's integer second filename convention\n",
    "        start_i = int(cfg.segment_duration * i)\n",
    "        end_i = int(cfg.segment_duration * (i + 1))\n",
    "        filename = f\"{stem}_{start_i:03d}_{end_i:03d}.wav\"\n",
    "\n",
    "        overlapping: list[int] = []\n",
    "        for idx, (onset, offset) in enumerate(events):\n",
    "            if _event_overlaps(float(onset), float(offset), seg_start, seg_end, cfg):\n",
    "                overlapping.append(idx)\n",
    "\n",
    "        if overlapping:\n",
    "            labels_here = _unique_preserve_order([event_labels[j] for j in overlapping])\n",
    "            label = cfg.label_separator.join(labels_here)\n",
    "            has_event = True\n",
    "            seg_events: list[list[float]] = []\n",
    "            seg_event_labels: list[str] = []\n",
    "            for j in overlapping:\n",
    "                onset, offset = events[j]\n",
    "                rel_on = max(0.0, float(onset) - seg_start)\n",
    "                rel_off = min(cfg.segment_duration, float(offset) - seg_start)\n",
    "                seg_events.append([rel_on, rel_off])\n",
    "                seg_event_labels.append(event_labels[j])\n",
    "        else:\n",
    "            label = cfg.no_event_label\n",
    "            has_event = False\n",
    "            seg_events = []\n",
    "            seg_event_labels = []\n",
    "\n",
    "        segment_rows.append(\n",
    "            {\n",
    "                \"filename\": filename,\n",
    "                \"original_filepath\": row[\"source_audio_path\"],\n",
    "                \"original_filename\": row[\"source_filename\"],\n",
    "                \"flat_filename\": row[\"flat_filename\"],\n",
    "                \"segment_start\": seg_start,\n",
    "                \"segment_end\": seg_end,\n",
    "                \"label\": label,\n",
    "                \"has_event\": has_event,\n",
    "                \"segment_events\": seg_events,\n",
    "                \"segment_event_labels\": seg_event_labels,\n",
    "                \"segment_event_clusters\": [],\n",
    "                \"dataset\": row[\"dataset\"],\n",
    "                \"split\": row[\"split\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "segment_df = pd.DataFrame(segment_rows)\n",
    "print(f\"Segments: {len(segment_df):,}\")\n",
    "segment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata.csv (JSON-encode list-like fields)\n",
    "csv_df = segment_df.copy()\n",
    "for col in [\"segment_events\", \"segment_event_labels\", \"segment_event_clusters\"]:\n",
    "    if col in csv_df.columns:\n",
    "        csv_df[col] = csv_df[col].apply(lambda x: json.dumps(convert_for_json(x)))\n",
    "csv_df.to_csv(DATASET_PATH / \"metadata.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Save labels.csv (preserve official split)\n",
    "labels_df = pd.DataFrame(\n",
    "    {\n",
    "        \"filename\": segment_df[\"filename\"],\n",
    "        \"label\": segment_df[\"label\"],\n",
    "        \"validation\": segment_df[\"split\"].eq(\"validation\"),\n",
    "    }\n",
    " )\n",
    "labels_df.to_csv(DATASET_PATH / \"labels.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Wrote: {DATASET_PATH / 'metadata.csv'}\")\n",
    "print(f\"Wrote: {DATASET_PATH / 'labels.csv'}\")\n",
    "print(f\"Train segments: {(~labels_df['validation']).sum():,}\")\n",
    "print(f\"Val segments:   {labels_df['validation'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d47f0d",
   "metadata": {},
   "source": [
    "## Verify output structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdbad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output directory structure:\")\n",
    "print(f\"  {DATASET_PATH}/\")\n",
    "print(f\"  ├── audio_flat/ ({len(list(FLAT_AUDIO.glob('*.wav')))} files)\")\n",
    "print(f\"  ├── data/{MODEL}/ ({len(list(SEG_PATH.glob('*.wav')))} files)\")\n",
    "print(f\"  ├── embeddings/{MODEL}/ ({len(list(EMB_PATH.glob('*.npy')))} files)\")\n",
    "print(\"  ├── labels.csv\")\n",
    "print(\"  └── metadata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
